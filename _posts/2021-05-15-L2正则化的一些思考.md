---
layout: post
title: "L2正则化的一些思考"
subtitle: '正则化'
author: "YiKe"
header-style: text
tags:
- impl
---


### L约束与泛化

#### 扰动敏感

记输入为$x$，输出为$y$，模型为$f$，模型参数为$\\theta$，记为：

$$ y = f\_{\\theta}(x)\\tag{1} $$

很多时候，我们希望得到一个"稳健"的模型。何为稳健？一般来说有两种含义，一是**对于参数扰动的稳定性**，比如模型变成了$f\_{\\theta + \\Delta \\theta}(x)$后是否还能达到相近的效果？而且还要考虑模型最终是否能恢复到$f\_{\\theta}(x)$；二是**对于输入扰动的稳定性**，比如输入从$x$变成了$x+\\Delta x$后，$f\_{\\theta}(x+\\Delta x)$是否能给出相近的预测结果。读者或许已经听过深度学习模型存在"对抗攻击样本"，比如图片只改变一个像素就给出完全不一样的分类结果，这就是模型对输入过于敏感的案例

#### L约束

所以，大多数时候我们都希望模型对输入扰动是不敏感的，这通常能提高模型的泛化性能。也就是说，我们希望$\\Vert x\_1-x\_2\\Vert$很小时

$$ \\begin{equation}\\Vert f\_{\\theta}(x\_1) - f\_{\\theta}(x\_2)\\Vert\\tag{2}\\end{equation} $$

也尽可能地小。当然，"尽可能"究竟是怎样，谁也说不准。于是Lipschitz提出了一个更具体的约束，那就是存在某个常数$C$（它只与参数有关，与输入无关），使得下式恒成立

$$ \\begin{equation}\\Vert f\_{\\theta}(x\_1) - f\_\\theta(x\_2)\\Vert\\leq C(\\theta)\\cdot \\Vert x\_1 - x\_2 \\Vert\\tag{3}\\end{equation} $$

也就是说，希望整个模型被一个线性函数"控制"住。这便是**L约束**

**换言之，在这里我们认为满足L约束的模型才是一个好模型，并且对于具体的模型，我们希望估算出$C(\\theta)$的表达式，并且希望$C(\\theta)$越小越好，越小意味着它对输入扰动越不敏感，泛化性越好**

#### 神经网络

在这里我们对具体的神经网络进行分析，以观察神经网络在什么时候会满足L约束

简单起见，我们考虑单层的全连接$f(Wx+b)$，这里的$f$是激活函数，而$W,b$则是参数矩阵/向量，这时(3)变为

$$ \\begin{equation}\\Vert f(Wx\_1+b) - f(Wx\_2+b)\\Vert\\leq C(W,b)\\cdot \\Vert x\_1 - x\_2 \\Vert\\tag{4}\\end{equation} $$

让$x\_1,x\_2$充分接近，那么就可以将左边用一阶项近似，得到

$$ \\begin{equation}\\left\\Vert \\frac{\\partial f}{\\partial x}W(x\_1 - x\_2)\\right\\Vert\\leq C(W,b)\\cdot \\Vert x\_1 - x\_2 \\Vert\\tag{5}\\end{equation} $$

> 这里就需要再次回顾一下高等数学中求导公式
>
> $$ \\frac{f(x+\\Delta x) - f(x)}{\\Delta x} = f'(x) $$
>
> 不知道大家有没有想过分母为什么是$\\Delta x$，实际上是由于分子两个$f$的参数相减得到的，那么类比我们就可以得到
>
> $$ \\frac{f(x + x\_1)-f(x+x\_2)}{x\_1-x\_2} = f'(x) $$
>
> 最终回到式(5)
>
> $$ \\frac{f(Wx\_1+b)-f(Wx\_2+b)}{W(x\_1-x\_2)}=f'(x) $$

显然，我们希望左边不超过右边，$\\frac{\\partial f}{\\partial x}$**这一项（每个元素）的绝对值必须不超过某个常数。这就要求我们使用"导数有上下界"的激活函数，不过我们目前常用的激活函数，比如sigmoid、tanh、relu等，都满足这个条件。**假定激活函数的梯度已经有界，尤其是我们常用的relu激活函数来说，这个界还是1。因此$\\frac{\\partial f}{\\partial x}$这一项只带来一个常数，我们暂时忽略它，接下来我们只需要考虑$\\Vert W(x\_1-x\_2)\\Vert$

多层的神经网络可以逐步递归分析，从而最终还是单层的神经网络问题，而CNN、RNN等结构本质上还是特殊的全连接，所以照样可以用全连接的结果。因此，对于神经网络来说，问题变成了：如果

$$ \\begin{equation}\\Vert W(x\_1 - x\_2)\\Vert\\leq C(W, b)\\cdot \\Vert x\_1 - x\_2 \\Vert\\tag{6}\\end{equation} $$

恒成立，那么$C$的表达式是什么？找出$C$的表达式后，我们就可以希望$C$尽可能小，从而给参数带来一个正则化项$C^2$

### 矩阵范数

#### 定义

其实到这里，我们已经将问题转化为了一个矩阵范数问题（矩阵范数的作用相当于向量的模长），它定义为

$$ \\begin{equation}\\Vert W\\Vert\_2 = \\max\_{x\\neq 0}\\frac{\\Vert Wx\\Vert}{\\Vert x\\Vert}\\label{eq:m-norm}\\tag{7}\\end{equation} $$

> 式(7)中的$x$实际上可以看作是$(x\_1-x\_2)$，那么有
>
> $$ C = \\frac{\\Vert W(x\_1-x\_2)\\Vert}{\\Vert x\_1-x\_2\\Vert} = \\Vert W\\Vert\_2 $$

如果$W$是一个方阵，那么该范数又称为"谱范数"，在本文中就算它不是方阵我们也叫它"谱范数"好了。注意$\\Vert Wx\\Vert$和$\\Vert x\\Vert$指的都是向量的范数，就是普通的向量模长。有了向量范数的概念后，我们就有

$$ \\begin{equation}\\Vert W(x\_1 - x\_2)\\Vert\\leq \\Vert W\\Vert\_2\\cdot\\Vert x\_1 - x\_2 \\Vert\\tag{8}\\end{equation} $$

其实也没做啥，就换了个记号而已，将$C$换为$\\Vert W\\Vert\_2$，而$\\Vert W\\Vert\_2$等于多少我们还是没有搞出来

#### Frobenius范数

其实谱范数$\\Vert W\\Vert\_2$的准确概念和计算方法要用到比较多的线性代数的概念，我们暂时不研究它，而是先研究一个更简单的范数：Frobenius范数，简称F范数。它的定义特别简单

$$ \\begin{equation}\\Vert W\\Vert\_F = \\sqrt{\\sum\_{i,j}w\_{ij}^2}\\tag{9}\\end{equation} $$

说白了就是直接把矩阵当成一个向量，然后求向量的欧式模长。简单通过柯西不等式，我们就能证明

$$ \\begin{equation}\\Vert Wx\\Vert\\leq \\Vert W\\Vert\_F\\cdot\\Vert x \\Vert\\tag{10}\\end{equation} $$

很明显$\\Vert W\\Vert\_F$提供了$\\Vert W\\Vert\_2$的一个上界，也就是说，你可以理解为$\\Vert W\\Vert\_2$是式(6)中最准确的$C$（所有满足式(6)的$C$中最小的那个），但如果你不太关心精准度，你可以直接取$C=\\Vert W\\Vert\_F$，也能使得(6)成立，毕竟$\\Vert W\\Vert\_F$容易计算

#### L2正则项

前面已经说过，为了使神经网络尽可能好的满足L约束，我们应当希望$C=\\Vert W\\Vert\_2$尽可能小，我们可以把$C^2$作为一个正则项加入到损失函数中。当然，我们还没有算出谱范数$\\Vert W\\Vert\_2$，但我们算出了一个更大的上界$\\Vert W\\Vert\_F$，那就先用着它吧，即loss为

$$ \\tilde{\\mathcal{L}} = \\mathcal{L}(y, f\_{\\theta}(x)) + \\lambda\\Vert W \\Vert\_F^2\\tag{11} $$

其中第一部分是指模型原来的loss。我们再来回顾一下$\\Vert W\\Vert\_F$的表达式，我们发现加入的正则项是

$$ \\begin{equation}\\lambda\\left(\\sum\_{i,j}w\_{ij}^2\\right)\\tag{12}\\end{equation} $$

这不就是L2正则化吗？终于，捣鼓了一番，**我们揭示了L2正则化（也称为weight decay）与L约束的联系，表明L2正则化能使得模型更好地满足L约束，从而降低模型对输入扰动的敏感性，增强模型的泛化性能**

#### Reference

+   [深度学习中的Lipschitz约束：泛化与生成模型](https://kexue.fm/archives/6051)
+   [Spectral Norm Regularization for Improving the Generalizability of Deep Learning](https://arxiv.org/abs/1705.10941)
+   [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957)