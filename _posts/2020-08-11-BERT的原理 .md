---
layout: post
title: "BERT 的 PyTorch 实现"
subtitle: ''
author: "YiKe"
header-style: text
tags:
- impl
---
首先我会详细阐述BERT原理，然后简单介绍一下ELMO以及GPT

### BERT详解

BERT全称为**Bidirectional Encoder Representation from Transformer**，是Google以无监督的方式利用大量**无标注**文本「炼成」的语言模型，其架构为Transformer中的Encoder（BERT=Encoder of Transformer）



以往为了解决不同的NLP任务，我们会为该任务设计一个最合适的神经网络架构并做训练，以下是一些简单的例子

![](https://s1.ax1x.com/2020/07/20/UfBBCj.jpg#shadow)

不同的NLP任务通常需要不同的模型，而设计这些模型并测试其performance是非常耗成本的（人力，时间，计算资源）。**如果有一个能直接处理各式NLP任务的通用架构该有多好？**

随着时代演进，不少人很自然地有了这样子的想法，而BERT就是其中一个将此概念付诸实践的例子

Google在预训练BERT时让它同时进行两个任务：

![](https://s1.ax1x.com/2020/07/20/UfDFZ8.png#shadow)

1.  漏字填空（完型填空），学术点的说法是 **Masked Language Model**
2.  判断第2个句子在原始本文中是否跟第1个句子相接（**Next Sentence Prediction**）

对正常人来说，要完成这两个任务非常简单。只要稍微看一下**前后文**就知道完形填空任务中`[MASK]`里应该填`退了`；而`醒醒吧`后面接`你没有妹妹`也十分合理

接下来我会分别详细介绍论文中这两个任务的设计细节

#### BERT语言模型任务一：Masked Language Model

在BERT中，Masked LM（Masked Language Model）构建了语言模型，简单来说，就是**随机遮盖或替换**一句话里面的任意字或词，然后让模型通过上下文预测那一个被遮盖或替换的部分，之后**做Loss的时候也只计算被遮盖部分的Loss**，这其实是一个很容易理解的任务，实际操作如下：

1.  随机把一句话中15%的token（字或词）替换成以下内容：

    1.  这些token有80%的几率被替换成`[MASK]`，例如my dog is hairy→my dog is \[MASK\]
    2.  有10%的几率被替换成任意一个其它的token，例如my dog is hairy→my dog is apple
    3.  有10%的几率原封不动，例如my dog is hairy→my dog is hairy
2.  之后让模型**预测和还原**被遮盖掉或替换掉的部分，计算损失的时候，只计算在第1步里被**随机遮盖或替换**的部分，其余部分不做损失，其余部分无论输出什么东西，都无所谓

这样做的好处是，BERT并不知道\[MASK\]替换的是哪一个词，而且任何一个词都有可能是被替换掉的，比如它看到的apple可能是被替换的词。这样强迫模型在编码当前时刻词的时候不能太依赖当前的词，而要考虑它的上下文，甚至根据上下文进行"纠错"。比如上面的例子中，模型在编码apple时，根据上下文my dog is，应该把apple编码成hairy的语义而不是apple的语义

#### BERT语言模型任务二：Next Sentence Prediction

我们首先拿到属于上下文的一对句子，也就是两个句子，之后我们要在这两个句子中加一些特殊的token：`[CLS]上一句话[SEP]下一句话[SEP]`。也就是在句子开头加一个`[CLS]`，在两句话之间和句末加`[SEP]`，具体地如下图所示  
![](https://s1.ax1x.com/2020/07/20/Ufs829.png#shadow)

可以看到，上图中的两句话明显是连续的。如果现在有这么一句话`[CLS]我的狗很可爱[SEP]企鹅不擅长飞行[SEP]`，可见这两句话就不是连续的。在实际训练中，我们会让这两种情况出现的数量为**1:1**

`Token Embedding`就是正常的词向量，即PyTorch中的`nn.Embedding()`

`Segment Embedding`的作用是用embedding的信息让模型分开上下句，我们给上句的token全0，下句的token全1，让模型得以判断上下句的起止位置，例如

```auto
[CLS]我的狗很可爱[SEP]企鹅不擅长飞行[SEP]
 0   0 0 0 0 0 0 0  1 1 1 1 1 1 1 1
```

`Position Embedding`和Transformer中的不一样，不是三角函数，而是学习出来的

#### Multi-Task Learning

BERT预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的Loss相加，例如

```auto
Input:
[CLS] calculus is a branch of math [SEP] panda is native to [MASK] central china [SEP]

Targets: false, south
----------------------------------
Input:
[CLS] calculus is a [MASK] of math [SEP] it [MASK] developed by newton and leibniz [SEP]

Targets: true, branch, was
```

#### Fine-Tuning

BERT的Fine-Tuning共分为4中类型，以下内容、图片均来自台大李宏毅老师[Machine Learning课程](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html)（以下内容 图在上，解释在下）

![](https://s1.ax1x.com/2020/07/20/UfRxJK.png#shadow)

如果现在的任务是**classification**，首先在输入句子的开头加一个代表分类的符号`[CLS]`，然后将该位置的output，丢给Linear Classifier，让其predict一个class即可。整个过程中Linear Classifier的参数是需要从头开始学习的，而BERT中的参数微调就可以了

这里李宏毅老师有一点没讲到，就是为什么要用第一个位置，即`[CLS]`位置的output。这里我看了网上的一些博客，结合自己的理解解释一下。因为BERT内部是Transformer，而Transformer内部又是Self-Attention，所以`[CLS]`的output里面肯定含有整句话的完整信息，这是毋庸置疑的。但是Self-Attention向量中，自己和自己的值其实是占大头的，现在假设使用$w_1$的output做分类，那么这个output中实际上会更加看重$w_1$，而$w_1$又是一个有实际意义的字或词，这样难免会影响到最终的结果。但是`[CLS]`是没有任何实际意义的，只是一个占位符而已，所以就算`[CLS]`的output中自己的值占大头也无所谓。当然你也可以将所有词的output进行concat，作为最终的output

![](https://s1.ax1x.com/2020/07/20/Uf7nqU.png#shadow)

如果现在的任务是**Slot Filling**，将句子中各个字对应位置的output分别送入不同的Linear，预测出该字的标签。其实这本质上还是个分类问题，只不过是对每个字都要预测一个类别

![](https://s1.ax1x.com/2020/07/20/Ufzq3D.png#shadow)

如果现在的任务是**NLI（自然语言推理）**。即给定一个前提，然后给出一个假设，模型要判断出这个假设是 正确、错误还是不知道。这本质上是一个三分类的问题，和Case 1差不多，对`[CLS]`的output进行预测即可

![](https://s1.ax1x.com/2020/07/20/UhSzM4.png#shadow)

如果现在的任务是QA（问答），举例来说，如上图，将一篇文章，和一个问题（这里的例子比较简单，答案一定会出现在文章中）送入模型中，模型会输出两个数s,e，这两个数表示，这个问题的答案，落在文章的第s个词到第e个词。具体流程我们可以看下面这幅图

![](https://s1.ax1x.com/2020/07/20/UhPMw9.png#shadow)

首先将问题和文章通过`[SEP]`分隔，送入BERT之后，得到上图中黄色的输出。此时我们还要训练两个vector，即上图中橙色和黄色的向量。首先将橙色和所有的黄色向量进行dot product，然后通过softmax，看哪一个输出的值最大，例如上图中$d_2$对应的输出概率最大，那我们就认为s=2

![](https://s1.ax1x.com/2020/07/20/UhP3Jx.png#shadow)

同样地，我们用蓝色的向量和所有黄色向量进行dot product，最终预测得$d_3$的概率最大，因此e=3。最终，答案就是s=2,e=3

你可能会觉得这里面有个问题，假设最终的输出s>e怎么办，那不就矛盾了吗？其实在某些训练集里，有的问题就是没有答案的，因此此时的预测搞不好是对的，就是没有答案

以上就是BERT的详细介绍，参考以下文章

+   [進擊的 BERT：NLP 界的巨人之力與遷移學習](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html)
+   [从零解读碾压循环神经网络的Transformer模型](https://github.com/aespresso/a_journey_into_math_of_ml/blob/master/04_transformer_tutorial_2nd_part/BERT_tutorial/transformer_2_tutorial.ipynb)
+   [李宏毅-Introduction of ELMO,BERT,GPT](https://www.bilibili.com/video/BV17441137fa)

### ELMo

ELMo是[Embedding from Language Model](https://arxiv.org/abs/1802.05365)的缩写，它通过**无监督**的方式对语言模型进行预训练来学习单词表示

这篇论文的想法其实非常简单，但是效果却很好。它的思路是用深度的双向Language Model在大量未标注数据上训练语言模型，如下图所示

![](https://s1.ax1x.com/2020/07/20/U4bEHf.png#shadow)

在实际任务中，对于输入的句子，我们使用上面的语言模型来处理它，得到输出向量，因此这可以看作是一种特征提取。但是ELMo与普通的Word2Vec或GloVe不同，ELMo得到的Embedding是有上下文信息的

具体来说，给定一个长度为N的句子，假设为$t_1,t_2,…,t_N$，语言模型会计算给定$t_1,t_2,…,t_{k-1}$的条件下出现$t_k$的概率：

$$ p(t_1,...,t_N)=\prod_{i=1}^{k}p(t_k\mid t_1,...,t_{k-1}) $$

传统的N-gram模型不能考虑很长的历史，因此现在的主流是使用多层双向LSTM。在时刻$k$，LSTM的第$j$层会输出一个隐状态$\overrightarrow{h_{kj}}$，其中$j=1,...,L$，$L$是LSTM的层数。最上层是$\overrightarrow{h_{kL}}$，对它进行softmax之后得到输出词的概率

类似地，我们可以用一个反向LSTM来计算概率：

$$ p(t_1,...,t_N)=\prod_{i=1}^{k}p(t_k\mid t_{k+1},...,t_N) $$

通过这个LSTM，我们可以得到$\overleftarrow{h_{kj}}$。我们的损失函数是这两个LSTM的加和

$$ \begin{aligned} \mathcal{L} = - \sum_{i=1}^n \Big( \log p(t_i \mid t_1, \dots, t_{i-1}; \Theta_e, \overrightarrow{\Theta}_\text{LSTM}, \Theta_s) + \ \log p(t_i \mid t_{i+1}, \dots, t_n; \Theta_e, \overleftarrow{\Theta}_\text{LSTM}, \Theta_s) \Big) \end{aligned} $$

这两个LSTM有各自的参数$\overrightarrow{\Theta}_{LSTM}$和$\overleftarrow{\Theta}_{LSTM}$，而Word Embedding参数$\Theta_x$和Softmax参数$\Theta_s$是共享的

#### ELMo Representations

对于输入的一个词$t_k$，ELMo会计算$2L+1$个representation（输入词的word embedding，前向后向的2L个representation）

$$ \begin{aligned} R_k&=\{x_k, \overrightarrow{h_{kj}}, \overleftarrow{h_{kj}}\mid j=1,2,...,L\} \ &=\{{h_{kj}}\mid j=0,1,...,L\} \end{aligned} $$

其中：

+   $h_{k0}$是词$t_k$的Embedding，上下文无关
+   $h_{kj}=\[\overrightarrow{h}_{kj}; \overleftarrow{h}_{kj}\], j>0$，上下文相关

为了用于下游（DownStream）的特定任务，我们会把不同层的隐状态组合起来，具体组合的参数是根据不同的特定任务学习出来的，公式如下：

$$ ELMo_k^{task}=E(R_k;\Theta_{task})=\gamma^{task}\sum_{j=0}^{L}s_j^{task}h_{kj} $$

这里的$\gamma^{task}$是一个缩放因子，而$s_j^{task}$用于把不同层的输出**加权组合**。在处理特定任务时，LSTM的参数$h_{kj}$都是固定的（或者是微调的），主要调的参数只是$\gamma^{task}$和$s_j^{task}$，当然这里ELMo只是一个特征提取，实际任务会再加上一些其它的网络架构

### GPT（Generative Pre-training Transformer）

GPT得到的语言模型参数不是固定的，它会根据特定的任务进行调整（通常是微调），这样的到的句子表示能更好的适配特定任务。它的思想也很简单，使用**单向Transformer**学习一个语言模型，对句子进行无监督的Embedding，然后根据具体任务对Transformer的参数进行微调。GPT与ELMo有两个主要的区别：

1.  模型架构不同：ELMo是浅层的双向RNN；GPT是多层的transformer decoder
2.  针对下游任务的处理不同：ELMo将词嵌入添加到特定任务中，作为附加功能；GPT则针对所有任务微调相同的基本模型

#### 无监督的Pretraining

这里解释一下上面提到的**单向Transformer**。在Transformer的文章中，提到了Encoder与Decoder使用的Transformer Block是不同的。在Decoder Block中，使用了Masked Self-Attention，即句子中的每个词都只能对包括自己在内的前面所有词进行Attention，这就是单向Transformer。GPT使用的Transformer结构就是将Encoder中的Self-Attention替换成了Masked Self-Attention，具体结构如下图所示

![](https://s1.ax1x.com/2020/07/21/U5Hjdf.png#shadow)

具体来说，给定一个未标注的预料库$\mathcal{U}=\{u_1,…,u_n\}$，我们训练一个语言模型，对参数进行最大（对数）似然估计：

$$ L_1(\mathcal{U})=\sum_i \log P(u_i|u_1,...,u_{k-1};\Theta) $$

训练的过程也非常简单，就是将n个词的**词嵌入($W_e$)加上位置嵌入($W_p$)**，然后输入到Transformer中，n个输出分别预测该位置的下一个词

$$ \begin{split} h_0 & =UW_e+W_p \ h_l & = \text{transformer_block}(h_{l-1}) \ P(u) & = \text{softmax}(h_n W_e^T) \end{split} $$

这里的位置编码没有使用传统Transformer固定编码的方式，而是动态学习的

#### 监督的Fine-Tuning

Pretraining之后，我们还需要针对特定任务进行Fine-Tuning。假设监督数据集合$\mathcal{C}$的输入$X$是一个词序列$x^1,...,x^m$，输出是一个分类的标签$y$，比如情感分类任务

我们把$x^1,...,x^m$输入Transformer模型，得到最上层最后一个时刻的输出$h_l^m$，将其通过我们新增的一个Softmax层（参数为$W_y$）进行分类，最后用CrossEntropyLoss计算损失，从而根据标准数据调整Transformer的参数以及Softmax的参数$W_y$。这等价于最大似然估计：

$$ \begin{aligned} &L_2(\mathcal{C})=\sum_{(x,y)}\log P(y\mid x^1,...,x^m) \ &P(y\mid x^1,...,x^m)=\text{softmax}(h_l^mW_y) \end{aligned} $$

正常来说，我们应该调整参数使得$L_2$最大，但是为了提高训练速度和模型的泛化能力，我们使用Multi-Task Learning，同时让它最大似然$L_1$和$L_2$

$$ L_3(\mathcal{C})=L_2(\mathcal{C})+\lambda \times L_1(\mathcal{C}) $$

这里使用的$L_1$还是之前语言模型的损失（似然），但是使用的数据不是前面无监督的数据$\mathcal{U}$，而是使用当前任务的数据$\mathcal{C}$，而且只使用其中的$X$，而不需要标签$y$

#### 其它任务

针对不同任务，需要简单修改下输入数据的格式，例如对于相似度计算或问答，输入是两个序列，为了能够使用GPT，我们需要一些特殊的技巧把两个输入序列变成一个输入序列

![](https://s1.ax1x.com/2020/07/21/U5X48U.png#shadow)

+   Classification：对于分类问题，不需要做什么修改
+   Entailment：对于推理问题，可以将先验与假设使用一个分隔符分开
+   Similarity：对于相似度问题，由于模型是单向的，但相似度与顺序无关，所以要将两个句子顺序颠倒后，把两次输入的结果相加来做最后的推测
+   Multiple-Choice：对于问答问题，则是将上下文、问题放在一起与答案分隔开，然后进行预测

### ELMo、GPT的问题

ELMo和GPT最大的问题就是传统的语言模型是单向的——我们根据之前的历史来预测当前词。但是我们不能利用后面的信息。比如句子`The animal didn’t cross the street because it was too tired`。我们在编码`it`的语义的时候需要同时利用前后的信息，因为在这个句子中，`it`可能指代`animal`也可能指代`street`。根据`tired`，我们推断它指代的是`animal`。但是如果把`tired`改成`wide`，那么`it`就是指代`street`了。传统的语言模型，都只能利用单方向的信息。比如前向的RNN，在编码`it`的时候它看到了`animal`和`street`，但是它还没有看到`tired`，因此它不能确定`it`到底指代什么。如果是后向的RNN，在编码的时候它看到了`tired`，但是它还根本没看到`animal`，因此它也不能知道指代的是`animal`。Transformer的Self-Attention理论上是可以同时关注到这两个词的，但是根据前面的介绍，为了使用Transformer学习语言模型，必须用Mask来让它看不到未来的信息，所以它也不能解决这个问题的

注意：即使ELMo训练了双向的两个RNN，但是一个RNN只能看一个方向，因此也是无法"同时"利用前后两个方向的信息的。也许有的读者会问，我的RNN有很多层，比如第一层的正向RNN在编码`it`的时候编码了`animal`和`street`的语义，反向RNN编码了`tired`的语义，然后第二层的RNN就能同时看到这两个语义，然后判断出`it`指代animal。理论上是有这种可能，但是实际上很难。举个反例，理论上一个三层（一个隐层）的全连接网络能够拟合任何函数，那我们还需要更多层的全连接网络或者CNN、RNN干什么呢？如果数据量不是及其庞大，或者如果不对网络结构做任何约束，那么它有很多种拟合的方法，其中大部分是过拟合的。但是通过对网络结构的约束，比如CNN的局部特效，RNN的时序特效，多层网络的层次结构，对它进行了很多约束，从而使得它能够更好的收敛到最佳的参数。我们研究不同的网络结构（包括Resnet、Dropout、BatchNorm等等）都是为了对网络增加额外的（先验的）约束

下图是从BERT论文中截取的三种模型的对比

![](https://s1.ax1x.com/2020/07/21/UISKpV.png#shadow)

### 参考文章

+   [Generalized Language Models](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html)
+   [BERT模型详解](http://fancyerii.github.io/2019/03/09/bert-theory/)
+   [词向量-ELMo介绍](https://akeeper.space/blog/582.html)