---
layout: post
title: "深度学习「黑话」解释"
subtitle: ''
author: "YiKe"
header-style: text
tags:
- impl
---




最近在和研一学弟学妹交流的时候，发现它们对深度学习中一些非常基础的概念不是很理解，于是我想做一个汇总，大家有认为不太好理解的概念，可以在评论区提出，我会逐渐添加到这篇文章中

+   端到端（end-to-end，end2end）：不需要做其他额外处理，从原始数据输入到任务结果输出，整个训练和预测过程，都是在模型里完成的
+   收敛：一般指的是某些指标（例如损失、准确率等）不再大幅度波动、震荡。例如你看到训练集的loss曲线呈平稳下降，我们就说，训练集loss正在慢慢收敛
+   分布（distribution）：分布不是让你真的去算这个一系列数据属于正态分布还是均匀分布等，而是一种大致的感觉，例如别人和你说，我们的数据分布特别不均衡，这个人所说的数据分布指的就是各个类别样本的数量比例；再比如别人和你说，这个样本的预测概率分布比较均匀，其实他的意思就是这个样本预测的概率向量比较均匀，例如一个二分类问题，那可能就是\[0.5,0.5\]这样，当然实际上这不是什么好的情况，因为这样我们就不知道这个样本到底该被预测为哪个类别了
+   维度（dimension）：维度分两种情况理解。假设一个人和你说：“这个数据的维度是2维的”，那其实就表示这个数据是一个矩阵；如果一个人和你说：“它的维度是128维”，那其实就表示矩阵中的某一个位置维度是128维的，比如一个句子经过WordEmbedding之后它的维度是\[batch\_size, seq\_len, emb\_dim\]，那么我说emb\_dim这个位置的维度是128维的
+   连续（continuous）：一般指的是向量中的值，例如经过WordEmbedding之后，向量里的值肯定都是连续的，例如\[1.271, -2.433,...\]这种
+   离散（discrete）：同样指的是向量中的值，例如经过WordEmbedding之前，向量里的值肯定都是离散的，因为是每个词对应的索引，例如\[0, 78, 32, 54, 1992,...\]这种

![Tipping QR Code](https://z3.ax1x.com/2021/03/25/6LchQK.jpg)
