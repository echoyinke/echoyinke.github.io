---
layout: post
title: "WEISSI 正则"
subtitle: ''
author: "YiKe"
header-style: text
tags:
- impl
---


L2正则的表现通常没有理论上说的那么好，很多时候加了可能还有负作用。最近的一篇文章[《Improve Generalization and Robustness of Neural Networks via Weight Scale Shifting Invariant Regularizations》](https://arxiv.org/abs/2008.02965)从"权重尺度偏移"这个角度分析了L2正则的弊端，并提出了新的WEISSI正则项。本文将指出常见的深度学习模型中存在的"权重尺度偏移(Weight Scale Shif)"现象，这个现象可能会导致L2正则的作用没那么明显。进一步地，我们可以构建一个新的正则项，它具有跟L2类似的作用，但是与权重尺度偏移现象更加协调，理论上来说更加有效。

#### 权重尺度偏移

我们知道深度学习模型的基本结构就是"线性变换+非线性激活函数"，而现在最常用的激活函数之一是$\text{ReLU}=\max(x, 0)$。有意思的是，这两者都满足"正齐次性"，也就是对于$\varepsilon \ge 0$，我们有$\varepsilon \phi(x)=\phi(\varepsilon x)$恒成立。对于其他的激活函数如SoftPlus、GELU、Swish等，其实它们都是$\text{ReLU}$的光滑近似，因此也可以认为它们是满足"正齐次性"

"正齐次性"使得深度学习模型对于权重尺度偏移具有一定的不变性。具体来说，假设一个$l$层的模型：

$$ \begin{aligned} \boldsymbol{h}_l =&\, \phi(\boldsymbol{W}_l \boldsymbol{h}_{l-1} + \boldsymbol{b}_l) \\ =& \,\phi(\boldsymbol{W}_l \phi(\boldsymbol{W}_{l-1} \boldsymbol{h}_{l-2} + \boldsymbol{b}_{l-1}) + \boldsymbol{b}_l) \\ =& \,\cdots\\ =& \,\phi(\boldsymbol{W}_l \phi(\boldsymbol{W}_{l-1} \phi(\cdots\phi(\boldsymbol{W}_1\boldsymbol{x} + \boldsymbol{b}_1)\cdots) + \boldsymbol{b}_{l-1}) + \boldsymbol{b}_l) \end{aligned}\tag{1} $$

假设每个参数引入偏移$\boldsymbol{W}_l = \gamma_l\tilde{\boldsymbol{W}}_l,\boldsymbol{b}_l = \gamma_l\tilde{\boldsymbol{b}}_l$，那么根据正齐次性可得

$$ \begin{aligned} \boldsymbol{h}_l =\left(\prod_{i=1}^l \gamma_i\right) \phi(\tilde{\boldsymbol{W}}_l \phi(\tilde{\boldsymbol{W}}_{l-1} \phi(\cdots\phi(\tilde{\boldsymbol{W}}_1\boldsymbol{x} + \tilde{\boldsymbol{b}}_1)\cdots) + \tilde{\boldsymbol{b}}_{l-1}) + \tilde{\boldsymbol{b}}_l) \end{aligned}\tag{2} $$

如果$\prod\limits_{i=1}^l \gamma_i=1$，那么参数为$\{\boldsymbol{W}_l,b_l\}$就跟参数完全等价了。换句话说，模型对于$\prod\limits_{i=1}^l\gamma_i=1$的权重尺度偏移具有不变性（WEIght-Scale-Shift-Invariance，WEISSI）

#### 与L2正则不协调

刚才我们说只要尺度偏移满足$\prod\limits_{i=1}^l\gamma_i=1$，那么两组参数对应的模型就等价了，但问题是它们对应的L2正则却不等价：

$$ \begin{equation}\sum_{i=1}^l \Vert\boldsymbol{W}_i\Vert_2^2=\sum_{i=1}^l \gamma_i^2\Vert\tilde{\boldsymbol{W}}_i\Vert_2^2\neq \sum_{i=1}^l \Vert\tilde{\boldsymbol{W}}_i\Vert_2^2\end{equation}\tag{3} $$

并且可以证明，如果固定$\Vert \boldsymbol{W}_1\Vert_2,\Vert \boldsymbol{W}_2\Vert_2,...,\Vert \boldsymbol{W}_l\Vert_2$，并且保持约束$\prod\limits_{i=1}^l\gamma_i=1$，那么$\sum\limits_{i=1}^l\left\Vert \tilde{\boldsymbol{W}}_i\right\Vert_2^2$的最小值在

$$ \begin{equation}\Vert\tilde{\boldsymbol{W}_1}\Vert_2^2=\Vert\tilde{\boldsymbol{W}}_2\Vert_2^2=\dots=\Vert\tilde{\boldsymbol{W}}_l\Vert_2^2=l\cdot\left(\prod_{i=1}^l \Vert\boldsymbol{W}_i\Vert_2^2\right)^{1/l}\end{equation}\tag{4} $$

> 上面的证明主要利用**积定和最小**性质（$a+b\ge 2\sqrt{ab}$变形），即已知$x>0,y>0$，则：
>
> 如果积$xy$是定值$p$，那么当且仅当$x=y$时，$x+y$有最小值$2\sqrt{xy}$。实际上我们将其推广到$l$项也是成立的。其中：
>
> $$ x_1 = \Vert\tilde{\boldsymbol{W}}_1\Vert_2^2=\frac{\Vert\boldsymbol{W}_1\Vert_2^2}{\gamma_1^2}\\ x_2 = \Vert\tilde{\boldsymbol{W}}_2\Vert_2^2=\frac{\Vert\boldsymbol{W}_2\Vert_2^2}{\gamma_2^2}\\ \vdots\\ x_l = \Vert\tilde{\boldsymbol{W}}_l\Vert_2^2=\frac{\Vert\boldsymbol{W}_l\Vert_2^2}{\gamma_l^2}\\ $$
>
> 因为
>
> $$ x_1x_2\cdots x_l=\prod_{i=1}^{l}\Vert\tilde{\boldsymbol{W}}_i\Vert_2^2=\frac{\prod\limits_{i=1}^l\Vert \boldsymbol{W}_i\Vert_2^2}{\prod\limits_{i=1}^{l}\gamma_i^2}=\prod_{i=1}^l\Vert \boldsymbol{W}_i\Vert_2^2 $$
>
> 是定值，所以当
>
> $$ \Vert\tilde{\boldsymbol{W}}_1\Vert_2^2=\Vert\tilde{\boldsymbol{W}}_2\Vert_2^2=\cdots =\Vert\tilde{\boldsymbol{W}}_l\Vert_2^2 $$
>
> 时，$\sum\limits_{i=1}^l\left\Vert \tilde{\boldsymbol{W}}_i\right\Vert_2^2$的最小值为
>
> $$ l\cdot\left(\prod_{i=1}^l \Vert\tilde{\boldsymbol{W}}_i\Vert_2^2\right)^{1/l}=l\cdot \left(\prod_{i=1}^l\Vert\boldsymbol{W}_i\Vert_2^2\right)^{1/l} $$

事实上，这就体现了L2正则的低效性。试想一下，假如我们已经训练得到一组参数$\{\boldsymbol{W}_l,b_l\}$，这组参数泛化性能可能不太好，于是我们希望L2正则能帮助优化器找到一组更好的参数（牺牲一点$\mathcal{L}_{\text{task}}$，降低一点$\mathcal{L}_{\text{reg}}$）。但是，上述结果告诉我们，由于权重尺度偏移不变性的存在，模型完全可以找到一组新的参数$\{\tilde{\boldsymbol{W}}_l, \tilde{b}_l\}$，它跟原来参数的模型完全等价（没有提升泛化性能），但是$\mathcal{L}_{\text{reg}}$还更小。说白了，就是L2正则确实起作用了，它使得$\sum\limits_{i=1}^l\Vert\boldsymbol{W}_i\Vert_2^2$更小，但并没有提升模型的泛化性能，没有达到使用L2正则的初衷

#### WEISSI正则

上述问题的根源在于，模型对权重尺度偏移具有不变性，但是L2正则对权重尺度偏移没有不变性。如果我们能找到一个新的正则项，它具有类似的作用，同时还对权重尺度偏移不变，那么就能解决这个问题了

我们考虑如下的一般形式的正则项

$$ \mathcal{L}_{\text{reg}}=\sum_{i=1}^l\varphi(\Vert\boldsymbol{W}_i\Vert_2)\tag{5} $$

对于L2正则来说，$\varphi(x)=x^2$，只要$\varphi(x)$是关于$x$在$\[0,+\infty)$上的单调递增函数，就能保证优化目标是缩小$\Vert\boldsymbol{W}_i\Vert$。要注意我们希望正则项具有尺度偏移不变性，其实并不需要严格要求$\varphi(\gamma x)=\varphi(x)$，而只需要

$$ \frac{d}{dx}\varphi(\gamma x)=\frac{d}{dx}\varphi(x)\tag{6} $$

因为优化过程只需要用到它的梯度。可能有的读者已经看出它的一个解了，其实就是对数函数$\varphi(x)=\log x$，所以新提出来的正则项就是

$$ \begin{aligned} \mathcal{L}_{\text{reg}}&=\sum_{i=1}^l \log \Vert \boldsymbol{W}_i\Vert_2\\ &=\log\left(\prod_{i=1}^l\Vert\boldsymbol{W}_i\Vert_2\right) \end{aligned}\tag{7} $$

除此之外，原论文可能担心上述正则项惩罚力度不够，因此还对参数方向加了个L1的惩罚，总的形式为：

$$ \mathcal{L}_{\text{reg}}=\lambda_1\sum_{i=1}^l\log \Vert\boldsymbol{W}_i\Vert_2+\lambda_2\sum_{i=1}^l\left\Vert \frac{\boldsymbol{W}_i}{\Vert\boldsymbol{W}_i\Vert_2}\right\Vert_2\tag{8} $$

#### 实验效果简述

按惯例展示一下原论文的实验结果，当然既然作者都整理成文了，显然说明是有积极效果的：

![](https://z3.ax1x.com/2021/06/13/2oER0g.png#shadow)

对我们来说，无非就是知道有这么个新的选择，炼丹的时候多一种尝试罢了。毕竟正则项这种东西，没有什么理论保证它一定能起作用，别人说得再漂亮也不一定有用，还是只有自己用了才知道效果

#### Reference

+   [L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸](https://kexue.fm/archives/7681)